{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule Events table extracted from page 10 and saved as /Users/petershmorhun/Desktop/SARScraper/SARPDFs/SARCSVs/AAG_Schedule.csv\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "# Open the PDF file using pdfplumber\n",
    "file_path = '/Users/petershmorhun/Desktop/SARScraper/SARPDFs/(U)AAG_MSAR_Dec_2023.pdf'\n",
    "\n",
    "def find_schedule_events_table(pdf):\n",
    "    # Iterate through the pages to find the \"(U) Schedule Events\" section\n",
    "    for page_num, page in enumerate(pdf.pages):\n",
    "        text = page.extract_text()\n",
    "        if \"(U) Schedule Events\" in text:\n",
    "            # If the target header is found, extract tables on that page\n",
    "            tables = page.extract_tables()\n",
    "            if tables:\n",
    "                # Assume the first table found is the desired one (based on typical format)\n",
    "                return tables[0], page_num\n",
    "    return None, None\n",
    "\n",
    "# Open the PDF and find the \"Schedule Events\" table\n",
    "with pdfplumber.open(file_path) as pdf:\n",
    "    schedule_table, schedule_page_num = find_schedule_events_table(pdf)\n",
    "\n",
    "# If the table is found, reformat it to the desired structure\n",
    "if schedule_table:\n",
    "    # Manually reformat the extracted table to match the structure\n",
    "    # Assuming the extracted table has a similar format as displayed in the example\n",
    "    headers = schedule_table[0]  # First row is considered the header\n",
    "    data_rows = schedule_table[1:]  # Remaining rows are the data\n",
    "    \n",
    "    # Create a DataFrame from the extracted data\n",
    "    df_schedule_events = pd.DataFrame(data_rows, columns=headers)\n",
    "\n",
    "    # Clean up the DataFrame to reflect the correct structure\n",
    "    df_schedule_events.columns = [\n",
    "        \"Events\", \"Type\", \"Objective (APB Change 1)\", \"Threshold (APB Change 1)\",\n",
    "        \"Current Estimate 12/31/2023\", \"Actual\"\n",
    "    ]\n",
    "    \n",
    "    # Save the DataFrame as a CSV file\n",
    "    file_name = '/Users/petershmorhun/Desktop/SARScraper/SARPDFs/SARCSVs/AAG_Schedule.csv'\n",
    "    df_schedule_events.to_csv(file_name, index=False)\n",
    "    \n",
    "    print(f\"Schedule Events table extracted from page {schedule_page_num + 1} and saved as {file_name}\")\n",
    "else:\n",
    "    print(\"Could not find the '(U) Schedule Events' table in the PDF.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Peformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Attributes table extracted from pages 11-12 and saved as /Users/petershmorhun/Desktop/SARScraper/SARPDFs/SARCSVs/AAG_Performance.csv\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "# Open the PDF file using pdfplumber\n",
    "#file_path = '/mnt/data/(U)AAG_MSAR_Dec_2023.pdf'\n",
    "\n",
    "def extract_performance_attributes_table(pdf, start_page, end_page):\n",
    "    # List to hold the extracted data\n",
    "    extracted_data = []\n",
    "\n",
    "    # Iterate through the specified page range\n",
    "    for page_num in range(start_page - 1, end_page):  # pdfplumber uses zero-based index\n",
    "        page = pdf.pages[page_num]\n",
    "        tables = page.extract_tables()\n",
    "        if tables:\n",
    "            # Assume the first table found on each page is part of the \"Performance Attributes\" table\n",
    "            extracted_data.extend(tables[0])  # Append rows from the first table\n",
    "\n",
    "    # Return the combined data\n",
    "    return extracted_data\n",
    "\n",
    "# Open the PDF and extract the \"Performance Attributes\" table from pages 11 to 12\n",
    "with pdfplumber.open(file_path) as pdf:\n",
    "    performance_attributes_data = extract_performance_attributes_table(pdf, 11, 12)\n",
    "\n",
    "# If data is found, format it into a DataFrame\n",
    "if performance_attributes_data:\n",
    "    # The first row should be the header, and the rest are data rows\n",
    "    headers = performance_attributes_data[0]\n",
    "    data_rows = performance_attributes_data[1:]\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    df_performance_attributes = pd.DataFrame(data_rows, columns=headers)\n",
    "\n",
    "    # Save the DataFrame as a CSV file\n",
    "    file_name = '/Users/petershmorhun/Desktop/SARScraper/SARPDFs/SARCSVs/AAG_Performance.csv'\n",
    "    df_performance_attributes.to_csv(file_name, index=False)\n",
    "\n",
    "    print(f\"Performance Attributes table extracted from pages 11-12 and saved as {file_name}\")\n",
    "else:\n",
    "    print(\"Could not find the 'Performance Attributes' table in the specified page range.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go with this:\n",
    "\n",
    "| Attribute | Current Estimate | Demonstrated Performance | APB Objective |  ABP Threshold | KPP/KSA |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| Aircraft Interoperability | Will meet threshold. Meets threshold requirements for C-2A, E- 2C, E-2D, F/A- 18E/F, EA- 18G and T- 45C. F-35C risk reduction testing conducted in FY 2022; follow-on compatibility testing with deadloads conducted in 2023; manned compatibility testing commenced in January 2024; Aircraft Recovery Bulletin (ARB) expected in FY 2024.  | Hookload limits and G-load limits demonstrated to be within limits as defined in ARB NO. 35-12 E. | The hookload limits and G-load limits applicable to each aircraft listed in the Development Threshold plus those listed in Table 2 shall not be exceeded when each aircraft engages the AAG at up to its maximum weight, net applied thrust, and maximum aircraft engaging velocity. | The hookload limits and G-load limits applicable to C-2A,E-2 Type/Model/Series (TMS), F/A-18, EA-18 TMS, F-35, and T45 aircraft shall not be exceeded when each aircraft engages the AAG at up to its maximum weight, net applied thrust, and maximum aircraft engaging velocity. | KPP |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "# Function to extract the \"Performance Attributes\" table from the PDF across pages 11 and 12\n",
    "def extract_performance_attributes_table(pdf, start_page, end_page):\n",
    "    extracted_data = []\n",
    "\n",
    "    # Iterate through the specified page range\n",
    "    for page_num in range(start_page - 1, end_page):\n",
    "        page = pdf.pages[page_num]\n",
    "        tables = page.extract_tables()\n",
    "        if tables:\n",
    "            extracted_data.extend(tables[0])  # Assume the first table found on each page is relevant\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "# Open the PDF file and extract data from pages 11 to 12\n",
    "#file_path = '/mnt/data/(U)AAG_MSAR_Dec_2023.pdf'\n",
    "with pdfplumber.open(file_path) as pdf:\n",
    "    raw_data = extract_performance_attributes_table(pdf, 11, 12)\n",
    "\n",
    "# Manually cleaning and structuring the extracted data\n",
    "structured_data = []\n",
    "current_attribute = None\n",
    "\n",
    "for i in range(0, len(raw_data), 2):  # Group rows in pairs for \"Objective\" and \"Threshold\"\n",
    "    if i + 1 < len(raw_data):\n",
    "        row1 = raw_data[i]\n",
    "        row2 = raw_data[i + 1]\n",
    "\n",
    "        # Set attribute name based on the first descriptive row\n",
    "        current_attribute = row1[0].split(\"\\n\")[0] if row1[0] else 'Unknown Attribute'\n",
    "\n",
    "        structured_data.append({\n",
    "            \"Attribute\": current_attribute,\n",
    "            \"Current Estimate\": row1[1] if len(row1) > 1 else None,\n",
    "            \"Demonstrated Performance\": row1[2] if len(row1) > 2 else None,\n",
    "            \"APB Objective\": row1[2] if len(row1) > 2 else None,\n",
    "            \"APB Threshold\": row2[2] if len(row2) > 2 else None,\n",
    "            \"KPP/KSA\": row2[3] if len(row2) > 3 else None\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_performance_attributes = pd.DataFrame(structured_data)\n",
    "\n",
    "# Display or save the DataFrame\n",
    "df_performance_attributes.to_csv('AAG_Performance_Attributes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table found on page 14 and saved as AAG_Total_Acquisition_Estimates.csv\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "def extract_table_after_signal(pdf, signal_text):\n",
    "    \"\"\"\n",
    "    Extract the first table found immediately after the specified signal text.\n",
    "    \"\"\"\n",
    "    for page_num, page in enumerate(pdf.pages):\n",
    "        text = page.extract_text()\n",
    "        if signal_text in text:\n",
    "            # Extract the tables from the current page\n",
    "            tables = page.extract_tables()\n",
    "            if tables:\n",
    "                # Assume the first table is the one following the signal text\n",
    "                return tables[0], page_num + 1  # Return the table and page number (1-based index)\n",
    "    return None, None\n",
    "\n",
    "# Open the PDF file and search for the table following \"(U) Total Acquisition Estimates and Quantities\"\n",
    "#file_path = '/mnt/data/(U)AAG_MSAR_Dec_2023.pdf'\n",
    "signal_text = '(U) Total Acquisition Estimates and Quantities'\n",
    "\n",
    "with pdfplumber.open(file_path) as pdf:\n",
    "    table_data, page_number = extract_table_after_signal(pdf, signal_text)\n",
    "\n",
    "# Process the table data if found\n",
    "if table_data:\n",
    "    # Convert the extracted table to a DataFrame\n",
    "    df_table = pd.DataFrame(table_data[1:], columns=table_data[0])  # First row as header\n",
    "    # Save the DataFrame to a CSV file\n",
    "    output_file = 'AAG_Total_Acquisition_Estimates.csv'\n",
    "    df_table.to_csv(output_file, index=False)\n",
    "    print(f\"Table found on page {page_number} and saved as {output_file}\")\n",
    "else:\n",
    "    print(\"Table not found with the given signal text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contracts data extracted and saved to Contracts_Data.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_contract_data(pdf, start_page, end_page):\n",
    "    \"\"\"\n",
    "    Extract contract information from pages specified (e.g., 22 and 23).\n",
    "    \"\"\"\n",
    "    extracted_data = []\n",
    "\n",
    "    # Iterate through the specified page range\n",
    "    for page_num in range(start_page - 1, end_page):  # pdfplumber uses zero-based indexing\n",
    "        page = pdf.pages[page_num]\n",
    "        text = page.extract_text()\n",
    "        tables = page.extract_tables()\n",
    "\n",
    "        # Example text parsing logic (adjust as needed to extract specific details)\n",
    "        if tables:\n",
    "            # Assuming one table per page based on the example\n",
    "            table = tables[0]\n",
    "            for row in table[1:]:  # Skip the header row\n",
    "                contract_name = row[0]  # Contract title\n",
    "                contract_number = row[1]  # Contract number\n",
    "                contractor = row[2]  # Contractor name\n",
    "                # Additional details would need to be parsed from the text\n",
    "\n",
    "                # Example structured data (using placeholders for now)\n",
    "                extracted_data.append({\n",
    "                    \"Contract Name\": contract_name,\n",
    "                    \"Contract Number\": contract_number,\n",
    "                    \"Contractor\": contractor,\n",
    "                    \"Contractor Location\": \"San Diego, CA\",  # Placeholder\n",
    "                    \"Contract Type\": \"Firm Fixed Price\",  # Placeholder\n",
    "                    \"Award Date\": \"2022-01-15\",  # Placeholder\n",
    "                    \"Definitization Date\": \"2022-02-01\",  # Placeholder\n",
    "                    \"Initial Contract Price Target\": 150000000.0,  # Placeholder\n",
    "                    \"Initial Contract Price Ceiling\": None,\n",
    "                    \"Initial Contract Quantity\": 0,\n",
    "                    \"Current Contract Price Target\": 175000000.0,  # Placeholder\n",
    "                    \"Current Contract Price Ceiling\": None,\n",
    "                    \"Current Contract Quantity\": 0,\n",
    "                    \"Contractor's Estimated Price at Completion\": 180000000.0,  # Placeholder\n",
    "                    \"PM's Estimated Price at Completion\": 185000000.0,  # Placeholder\n",
    "                    \"Cost Variance\": 8000000.0,  # Placeholder\n",
    "                    \"Schedule Variance\": -3000000.0,  # Placeholder\n",
    "                    \"Cost Variance Explanation\": \"Explanation about cost variance goes here...\",  # Placeholder\n",
    "                    \"Schedule Variance Explanation\": \"Explanation about schedule variance goes here...\",  # Placeholder\n",
    "                    \"Variance Explanation\": \"<div>Explanation about both variances goes here...</div>\",  # Placeholder\n",
    "                    \"Contract Comments\": None\n",
    "                })\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "# Open the PDF file and extract contract data from pages 22 and 23\n",
    "#file_path = '/mnt/data/(U)AAG_MSAR_Dec_2023.pdf'\n",
    "\n",
    "with pdfplumber.open(file_path) as pdf:\n",
    "    contracts_data = extract_contract_data(pdf, 22, 23)\n",
    "\n",
    "# Convert the extracted data to a DataFrame and save it to a CSV file\n",
    "df_contracts = pd.DataFrame(contracts_data)\n",
    "output_file = 'Contracts_Data.csv'\n",
    "df_contracts.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Contracts data extracted and saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 55\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Open the PDF file and extract LRIP data from page 24\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m#file_path = '/mnt/data/(U)AAG_MSAR_Dec_2023.pdf'\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pdfplumber\u001b[38;5;241m.\u001b[39mopen(file_path) \u001b[38;5;28;01mas\u001b[39;00m pdf:\n\u001b[0;32m---> 55\u001b[0m     lrip_data \u001b[38;5;241m=\u001b[39m extract_lrip_data(pdf, \u001b[38;5;241m24\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Convert the extracted data to a DataFrame and save it to a CSV file\u001b[39;00m\n\u001b[1;32m     58\u001b[0m df_lrip \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(lrip_data)\n",
      "Cell \u001b[0;32mIn[15], line 20\u001b[0m, in \u001b[0;36mextract_lrip_data\u001b[0;34m(pdf, page_num)\u001b[0m\n\u001b[1;32m     18\u001b[0m initial_quantity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39misdigit() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     19\u001b[0m initial_reference \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m---> 20\u001b[0m initial_start_year \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m     21\u001b[0m initial_end_year \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;241m4\u001b[39m]\n\u001b[1;32m     22\u001b[0m current_approval_date \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;241m5\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "def extract_lrip_data(pdf, page_num):\n",
    "    \"\"\"\n",
    "    Extract the Low-Rate Initial Production (LRIP) data from the specified page.\n",
    "    \"\"\"\n",
    "    page = pdf.pages[page_num - 1]  # pdfplumber uses zero-based indexing\n",
    "    tables = page.extract_tables()\n",
    "\n",
    "    if tables:\n",
    "        # Assume the first table found on the page is the LRIP table\n",
    "        table = tables[0]\n",
    "        # Parse the relevant details from the table\n",
    "        lrip_data = []\n",
    "        for row in table[1:]:  # Skip the header row\n",
    "            initial_approval_date = row[0]\n",
    "            initial_quantity = int(row[1]) if row[1].isdigit() else None\n",
    "            initial_reference = row[2]\n",
    "            initial_start_year = row[3]\n",
    "            initial_end_year = row[4]\n",
    "            current_approval_date = row[5]\n",
    "            current_quantity = int(row[6]) if row[6].isdigit() else None\n",
    "            current_reference = row[7]\n",
    "            current_start_year = row[8]\n",
    "            current_end_year = row[9]\n",
    "            notes = row[10] if len(row) > 10 else None\n",
    "\n",
    "            # Create a structured dictionary based on the desired output\n",
    "            lrip_data.append({\n",
    "                \"ID\": 2,  # Placeholder; will need to be set dynamically if used in a loop\n",
    "                \"SubmissionID\": 4,  # Placeholder\n",
    "                \"SubProgramID\": 285,  # Placeholder\n",
    "                \"InitialApprovalDate\": initial_approval_date,\n",
    "                \"InitialQuantity\": initial_quantity,\n",
    "                \"InitialReference\": initial_reference,\n",
    "                \"InitialStartYear\": initial_start_year,\n",
    "                \"InitialEndYear\": initial_end_year,\n",
    "                \"CurrentApprovalDate\": current_approval_date,\n",
    "                \"CurrentQuantity\": current_quantity,\n",
    "                \"CurrentReference\": current_reference,\n",
    "                \"CurrentStartYear\": current_start_year,\n",
    "                \"CurrentEndYear\": current_end_year,\n",
    "                \"Notes\": notes\n",
    "            })\n",
    "\n",
    "        return lrip_data\n",
    "\n",
    "    return []\n",
    "\n",
    "# Open the PDF file and extract LRIP data from page 24\n",
    "#file_path = '/mnt/data/(U)AAG_MSAR_Dec_2023.pdf'\n",
    "\n",
    "with pdfplumber.open(file_path) as pdf:\n",
    "    lrip_data = extract_lrip_data(pdf, 24)\n",
    "\n",
    "# Convert the extracted data to a DataFrame and save it to a CSV file\n",
    "df_lrip = pd.DataFrame(lrip_data)\n",
    "output_file = 'LRIP_Data.csv'\n",
    "df_lrip.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Low-Rate Initial Production data extracted and saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-Rate Initial Production data extracted and saved to LRIP_Data.csv\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "def extract_lrip_table(pdf, page_num):\n",
    "    \"\"\"\n",
    "    Extracts the Low-Rate Initial Production (LRIP) table from the specified page.\n",
    "    \"\"\"\n",
    "    page = pdf.pages[page_num - 1]  # Zero-based indexing\n",
    "    tables = page.extract_tables()\n",
    "\n",
    "    if tables:\n",
    "        # Assume the first table found on the page is the LRIP table\n",
    "        return tables[0]\n",
    "    return None\n",
    "\n",
    "def parse_lrip_table(raw_table):\n",
    "    \"\"\"\n",
    "    Parses the raw LRIP table data into the desired structured format.\n",
    "    \"\"\"\n",
    "    return [{\n",
    "        \"ID\": 2,  # Placeholder; can be dynamically assigned if necessary\n",
    "        \"SubmissionID\": 4,  # Placeholder; to be adjusted based on actual requirements\n",
    "        \"SubProgramID\": 285,  # Placeholder; replace with the correct SubProgramID if needed\n",
    "        \"InitialApprovalDate\": raw_table[2][1],  # Original Date\n",
    "        \"InitialQuantity\": int(raw_table[1][1]),  # Original Quantity\n",
    "        \"InitialReference\": raw_table[3][1],  # Original Reference\n",
    "        \"InitialStartYear\": raw_table[4][1].split(\" - \")[0],  # Start year from LRIP Period\n",
    "        \"InitialEndYear\": raw_table[4][1].split(\" - \")[1],  # End year from LRIP Period\n",
    "        \"CurrentApprovalDate\": raw_table[2][2],  # Current Date\n",
    "        \"CurrentQuantity\": int(raw_table[1][2]),  # Current Quantity\n",
    "        \"CurrentReference\": raw_table[3][2],  # Current Reference\n",
    "        \"CurrentStartYear\": raw_table[4][2].split(\" - \")[0],  # Start year from LRIP Period\n",
    "        \"CurrentEndYear\": raw_table[4][2].split(\" - \")[1],  # End year from LRIP Period\n",
    "        \"Notes\": None  # Assuming no notes are present\n",
    "    }]\n",
    "\n",
    "# Open the PDF file and extract LRIP data from page 24\n",
    "#file_path = '/mnt/data/(U)AAG_MSAR_Dec_2023.pdf'\n",
    "\n",
    "with pdfplumber.open(file_path) as pdf:\n",
    "    raw_table_page_24 = extract_lrip_table(pdf, 24)\n",
    "\n",
    "# Parse the table into the desired structured format\n",
    "if raw_table_page_24:\n",
    "    lrip_parsed_data = parse_lrip_table(raw_table_page_24)\n",
    "    # Convert to DataFrame and save to a CSV file\n",
    "    df_lrip_parsed = pd.DataFrame(lrip_parsed_data)\n",
    "    output_file = 'LRIP_Data.csv'\n",
    "    df_lrip_parsed.to_csv(output_file, index=False)\n",
    "    print(f\"Low-Rate Initial Production data extracted and saved to {output_file}\")\n",
    "else:\n",
    "    print(\"No table found on page 24.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "def make_columns_unique(columns):\n",
    "    \"\"\"\n",
    "    Ensure unique column names by appending suffixes if needed.\n",
    "    \"\"\"\n",
    "    seen = {}\n",
    "    for i, col in enumerate(columns):\n",
    "        if col in seen:\n",
    "            seen[col] += 1\n",
    "            columns[i] = f\"{col}_{seen[col]}\"\n",
    "        else:\n",
    "            seen[col] = 0\n",
    "    return columns\n",
    "\n",
    "def extract_tables_with_column_adjustment(pdf, pages):\n",
    "    \"\"\"\n",
    "    Extract tables from specified pages and handle column adjustments.\n",
    "    \"\"\"\n",
    "    all_tables = []\n",
    "    for page_num in pages:\n",
    "        page = pdf.pages[page_num - 1]  # Zero-based indexing for pages in pdfplumber\n",
    "        tables = page.extract_tables()\n",
    "\n",
    "        if tables:\n",
    "            # Extract the \"Code\" from the first row (if present)\n",
    "            code = tables[0][0][0] if tables[0] and len(tables[0][0]) > 0 else \"\"\n",
    "\n",
    "            # Get the header row and adjust if needed\n",
    "            header = make_columns_unique(tables[0][0])\n",
    "            num_columns = len(header)\n",
    "\n",
    "            # Process the remaining rows\n",
    "            data_rows = []\n",
    "            for row in tables[0][1:]:\n",
    "                # Ensure each row has the same number of columns as the header\n",
    "                if len(row) < num_columns:\n",
    "                    row.extend([None] * (num_columns - len(row)))  # Pad with None\n",
    "                elif len(row) > num_columns:\n",
    "                    row = row[:num_columns]  # Truncate to match the header length\n",
    "                data_rows.append(row)\n",
    "\n",
    "            # Create a DataFrame using the adjusted rows\n",
    "            df = pd.DataFrame(data_rows, columns=header)\n",
    "\n",
    "            # Add the \"Code\" column to every row\n",
    "            df[\"Code\"] = code\n",
    "\n",
    "            # Append the DataFrame to the list\n",
    "            all_tables.append(df)\n",
    "\n",
    "    return all_tables\n",
    "\n",
    "# Open the PDF file and extract tables from pages 40, 41, and 42\n",
    "file_path = '/mnt/data/(U)AAG_MSAR_Dec_2023.pdf'\n",
    "with pdfplumber.open(file_path) as pdf:\n",
    "    tables_list = extract_tables_with_column_adjustment(pdf, [40, 41, 42])\n",
    "\n",
    "# Combine the tables into a single DataFrame\n",
    "merged_df = pd.concat(tables_list, ignore_index=True)\n",
    "\n",
    "# Clean the merged DataFrame by replacing '-' with NaN\n",
    "merged_df.replace('-', pd.NA, inplace=True)\n",
    "\n",
    "# Output the cleaned DataFrame to a CSV file for inspection\n",
    "output_file_path = '/mnt/data/merged_cleaned_table.csv'\n",
    "merged_df.to_csv(output_file_path, index=False)\n",
    "print(f\"Data saved to {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
